{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyalbino/WITCON2024/blob/main/%5BWiTCon_2024%5D_Building_Solutions_with_RAG_Code_Along.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ùó **ACTION NEEDED:** Create your own COPY of this notebook\n",
        "\n",
        "üëâ This is a code-along version of the notebook. This does not contain all the lines of code.\n",
        "\n",
        "üëâ **The participant is tasked to fill in missing lines.** Have fun!"
      ],
      "metadata": {
        "id": "-fO_wPH8RrJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üÜò **NEED HELP?** If you get lost, you can check out the answer key: [the complete version of the notebook](https://colab.research.google.com/drive/1XmTDRis2kGHMyux3xgSnXWaxSSL3_cpw?usp=sharing)!"
      ],
      "metadata": {
        "id": "QeJcxgoWUtG5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2DnPt_ysc2y"
      },
      "source": [
        "# üåê Retrieval Augmented Generation\n",
        "\n",
        "**Retrieval Augmented Generation (RAG)** is a GenAI technique for applying large language models  (LLMs) to **specific use cases**.\n",
        "\n",
        "On a high-level, RAG equips an LLM with custom data ‚Äî typically found in documents ‚Äî to produce more accurate, relevant, and contextually rich responses than it could by relying only on its pre-trained knowledge.\n",
        "\n",
        "## **üçÉ Let's find a real-world problem that we can solve with RAG!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üì¢ *Special thanks to Aiden Dai! This notebook is built upon his [LangChain RAG Tutorial](https://github.com/daixba/langchain-tutorials/blob/main/02-langchain-rag.ipynb)!*"
      ],
      "metadata": {
        "id": "8TCqosk6DYR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† The Problem\n",
        "\n",
        "ü¶† The COVID-19 pandemic has led to an overwhelming volume of scientific papers, making it difficult for health experts and policymakers to catch up with the latest research."
      ],
      "metadata": {
        "id": "FqPMmCxG5Ua-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üí° The Solution\n",
        "\n",
        "ü§ñ To make the process of reviewing research papers faster, we can use a LLM that can answer medical questions given a research paper via RAG."
      ],
      "metadata": {
        "id": "MEV5tGyw9BQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üë®‚Äçüíª The Implementation"
      ],
      "metadata": {
        "id": "eqdKJBLGA_8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0Ô∏è‚É£ Install libraries"
      ],
      "metadata": {
        "id": "wefrqgFGIbJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip -qqq install langchain sentence-transformers faiss-cpu langchain-openai"
      ],
      "metadata": {
        "id": "TAg_LudmIaps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6mBeaq0sc29"
      },
      "source": [
        "## 1Ô∏è‚É£ Load a document that contains your custom data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfmZ1ky8sc29"
      },
      "source": [
        "For this specific use case, the document that we will load is a research paper related to COVID-19:\n",
        "[Effects of COVID-19 on College Students‚Äô Mental Health in the United States: Interview Survey Study](https://www.jmir.org/2020/9/e21279/).\n",
        "\n",
        "Of course, you can upload many other kinds of documents, such as PDFs, CSVs, etc. [Learn more here!](https://python.langchain.com/docs/modules/data_connection/document_loaders/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVabLPvjsc2-"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "# Find the URL of the web page\n",
        "url = \"<üí° ACTION NEEDED: Write down the URL of the web page>\"\n",
        "\n",
        "# Load a web page\n",
        "web_loader = WebBaseLoader(url)\n",
        "document = web_loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yezX3PzAsc2_"
      },
      "source": [
        "## 2Ô∏è‚É£ Split the document into chunks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking a document is necessary to break down texts into smaller and manageable pieces called \"chunks\".\n",
        "\n",
        "> üí° **What is a chunk?** A chunk serves as a useful unit of information that can be retrieved to answer specific questions about the text."
      ],
      "metadata": {
        "id": "_3RU-YvmF_--"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWC4Tsgzsc2_"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Split a long document into smaller chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\"],\n",
        "    chunk_size = 512,\n",
        "    chunk_overlap  = 0,\n",
        "    length_function = len,\n",
        "    is_separator_regex = False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XThxtGNsc2_"
      },
      "outputs": [],
      "source": [
        "# Split the documents into chunks\n",
        "chunks = \"<üí° ACTION NEEDED: Write down the function that splits the document into chunks>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tD8KNHgosc2_"
      },
      "outputs": [],
      "source": [
        "# Check how many chunks we have\n",
        "# \"<üí° ACTION NEEDED: Get the number of chunks>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6YXMqmksc2_"
      },
      "outputs": [],
      "source": [
        "# Randomly preview a chunk of the document\n",
        "# \"<üí° ACTION NEEDED: Get any element from the list of chunks>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQQjBXQxsc2_"
      },
      "source": [
        "## 3Ô∏è‚É£ Convert the chunks into vector embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxOIQ69hsc2_"
      },
      "source": [
        "> üí° **What are vector embeddings?** Vector embeddings are numerical representations of words that capture their meaning, enabling computers (or LLMs) to understand language.\n",
        "\n",
        "There are many kinds of vector embeddings to choose from! You can find a lot of these in [Hugging Face](https://huggingface.co/models?other=embeddings). Even [OpenAI](https://platform.openai.com/docs/guides/embeddings) has their own embeddings - but keep in mind that these models come with a price tag!\n",
        "\n",
        "**For this use case, we choose an OpenAI embedding model called `text-embedding-ada-002` for the experience!**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# üí° ACTION NEEDED: Run this cell and paste this API key: sk-proj-gn028j30woImOKOw3JZqT3BlbkFJx6TGXlBxQsmOE7EnEcpb\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "0DuXzT6RTTrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "# üí° ACTION NEEDED: Type in the embedding model we will be using\n",
        "# üÜò HINT: You can see it above!\n",
        "embeddings_model = OpenAIEmbeddings(\n",
        "    model=\"üí° ACTION NEEDED: Type in the embedding model we will be using\",\n",
        ")"
      ],
      "metadata": {
        "id": "aYhLGHmfTzWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOhpjGo7sc3A"
      },
      "outputs": [],
      "source": [
        "# Convert a sample phrase into an embedding\n",
        "sample_phrase = \"üí° ACTION NEEDED: Type in a sample phrase that we will convert into an embedding\"\n",
        "\n",
        "sample_embeddings = embeddings_model.embed_query(sample_phrase)\n",
        "sample_embeddings[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be converting all the chunks into vector embeddings in the next step!\n",
        "\n",
        "This process is already taken care of by the function that creates a vector store."
      ],
      "metadata": {
        "id": "u-ACsyM-KITd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RsSE7Yxsc3A"
      },
      "source": [
        "## 4Ô∏è‚É£ Store the vector embeddings in a vector store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IrVUk0jsc3A"
      },
      "source": [
        "> üí° **What is a vector store?** A vector store is like a massive filing system where pieces of information (chunks) represented as lists of numbers (vector embeddings) are organized in a way that makes it easy for computers to search and retrieve relevant data quickly.\n",
        "\n",
        "There are many vector stores supported by LangChain. [Learn more here!](https://python.langchain.com/docs/integrations/vectorstores/)\n",
        "\n",
        "For this use case, we will use FAISS, which stands for Facebook AI Similarity Search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RaUq1aJsc3A"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Convert the chunks into vector embeddings, and...\n",
        "# Store the vector embeddings in a FAISS vector store\n",
        "vector_store = FAISS.from_documents(\n",
        "  \"üí° ACTION NEEDED: Get the chunks\",\n",
        "  \"üí° ACTION NEEDED: Get the embedding model\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ Retrieve the chunks that can best answer a question"
      ],
      "metadata": {
        "id": "lyJKEtVULa4U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWzLk0-zsc3B"
      },
      "source": [
        "Given a question, we want to retrieve the chunks of information that can best answer it.\n",
        "\n",
        "We can choose from [many retrieval approaches](https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore/).\n",
        "\n",
        "For this use case, we will use an approach called `top k retrieval`, which simply retrieves the top k most relevant chunks of information given a question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkDbWCY-sc3B"
      },
      "outputs": [],
      "source": [
        "# Write a question about the document\n",
        "question = \"üí° ACTION NEEDED: Ask a question about the document\"\n",
        "\n",
        "# Instruct the retriever to get the top x chunks that seem most likely to answer our question\n",
        "k = \"üí° ACTION NEEDED: Select a number of chunks we want to retrieve\"\n",
        "\n",
        "# Create a retriever with the vector store\n",
        "retriever = \"üí° ACTION NEEDED: Create a retriever\"\n",
        "\n",
        "# Retrieve the chunks that can best answer a question\n",
        "retrieved_chunks = \"üí° ACTION NEEDED: Retrieve relevant chunks\"\n",
        "retrieved_chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2YFjiHSsc3C"
      },
      "source": [
        "## 6Ô∏è‚É£ Generate the answer using an LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GScE0RJjsc3C"
      },
      "source": [
        "After looking for chunks of information from the documents that are closely similar to what the query is asking for, we can head onto the next step.\n",
        "\n",
        "The next step is to take the question and the retrieved chunks in order to write a prompt that we can give to our LLM.\n",
        "\n",
        "Given the prompt, the LLM then produces an insightful answer to our question.\n",
        "\n",
        "> üí° **What is a prompt?** A prompt is an instruction that commands an LLM to perform a specific task."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many LLMs we can choose from! We can actually use [OpenAI's GPT models](https://platform.openai.com/docs/models), which power ChatGPT. Keep in mind however that these models come with a price tag.\n",
        "\n",
        "But do not worry! There are still a lot more [free-to-use LLMs](https://www.datacamp.com/blog/top-open-source-llms) out there. Many free LLMs can also be found on [Hugging Face ü§ó](https://huggingface.co/models?pipeline_tag=text-generation&sort=likes), which is a machine learning platform that aims to make AI tech accessible to everyone.\n",
        "\n",
        "**For this use case, we will be using OpenAI's `gpt-3.5-turbo` for the experience!**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3oi2rEctQLZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# üí° ACTION NEEDED: Run this cell and paste this API key: sk-proj-gn028j30woImOKOw3JZqT3BlbkFJx6TGXlBxQsmOE7EnEcpb\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "xFwasP9BMWk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zse95wWisc3C"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# üí° ACTION NEEDED: Type in the name of the LLM we will be using\n",
        "# üÜò HINT: You can see it above!\n",
        "llm = ChatOpenAI(model=\"üí° ACTION NEEDED: Type in the name of LLM the we will be using\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "# Create a prompt that contains the following:\n",
        "# (1) Task\n",
        "# (2) Context\n",
        "# (3) Question\n",
        "# (4) Output indicator\n",
        "prompt = \"\"\"\n",
        "üí° ACTION NEEDED: Create a prompt\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=prompt, input_variables=[\"context\", \"question\"],\n",
        "  )"
      ],
      "metadata": {
        "id": "wZZUOMCrW4pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Create a question-answering (QA) chain that can be used to answer questions ...\n",
        "# by retrieving relevant chunks of information from our vector store (retriever), ...\n",
        "# by using a prompt that states the task, context, and question, and ...\n",
        "# by using an LLM\n",
        "qa_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "kumf-FByNEon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = qa_chain.invoke(\"üí° ACTION NEEDED: Ask a question\")"
      ],
      "metadata": {
        "id": "rktK6v2MNZbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "width = 80\n",
        "\n",
        "# Format the answer\n",
        "answer = textwrap.fill(answer, width=width)\n",
        "\n",
        "# Print the answer\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "2OwEy8b6N3vc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}